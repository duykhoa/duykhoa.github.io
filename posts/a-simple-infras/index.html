<!doctype html><html><body><a href=/>Kevin Tran</a>
/ Blog<div id=content><h2>A Simple Infrastructure</h2>2022-06-90<nav id=TableOfContents><ul><li><a href=#intro>Intro</a></li><li><a href=#projects-setup>Projects setup</a></li><li><a href=#infrastructure-requirement>Infrastructure requirement</a></li><li><a href=#everything-starts-with-jenkins>Everything starts with Jenkins</a></li><li><a href=#ansible>Ansible</a></li><li><a href=#conclusion>Conclusion</a></li></ul></nav><h2 id=intro>Intro</h2><p>Nowadays, there are a variety of choices to deploy a website. Developers could choose a container service like Heroku, or go with the most trendy infrastructure of choice with Kubernetes, or use the Serverless strategy to run the website without the need of setting up the server by their own.
However, configuring a web server is no longer a rocket surgery with the modern Devops technologies and the revolution of the Cloud providers. This post explores in details a simple infrastructure setup with Amazon infrastructure using Terraform, Ansible and Jenkins.
This is a real usecase that fits to a certain software project, therefore it requires some consideration before implementing this setup in another organization.</p><h2 id=projects-setup>Projects setup</h2><p>To embrace the culture of Agile development, the platform is splitted into multiple small applications which is only taken a couple of sprints from the planning phrase until going live. With that setup, the monorepo is used in order to keep the consistency between these applications in term of development best practice, linting rules, scripting commands, version control. There are a bunch of shared libraries extracted from the apps for the common usages.</p><p>Depends on each individual app&rsquo;s purpose, the micro app could involve several technologies. So far the platform heavily uses GraphQL to provide the frontend clients to access. Not only it keeps the number of API as small as 1, it also allows the clients to specify the structure of the response they want to use. These advantages completely convince the team to switch over from the traditional REST (or RESTful) API. Besides GraphQL, GRPC is the main communication protocol from backend to backend services, which allows small apps to communicate with each other with an extremely fast latency.</p><p>Other than that, the rest of stack follows the typical setup of a web service. A database (postgresql or mysql) handles the data and a small Redis cluster that acts as a caching layer as well as backend for the jobs&rsquo; queue. Lastly, the application code is running in one or more server instances, which doesn&rsquo;t produce any state in the server and there could be multiple processes running the code in parralel.</p><h2 id=infrastructure-requirement>Infrastructure requirement</h2><p>The most difficult question when designing the infrastructure is how to maximize the support for the monorepo setup. Each app share a lot in common for the infrastructure requirements such as the configuration of server instances, load balancer, security groups beside a few differences in the exposed ports, protocol, database engine.</p><p>Terraform is the only option we have in mind when choosing the tools for building the infrastructure. Terraform comes with a lot of configurations in each resource type and requires user to know how to link these resources together. Not all members in the teams have deep knowledge about the cloud provider (we are using Amazon Web service) in order to write the code correctly. To solve this problem, we come up with a list of custom modules that combine multiple resources together, this helps to setup the infrastructure quickly and error-free. This example may provide a clearer picture of what a module looks like and how it helps to spin up the infrastructure at ease.</p><pre tabindex=0><code>  module &#34;server_instances&#34; {
    source                                 = &#34;../../../modules/server_instances&#34;
    instances_count                        = 3
    instance_type                          = &#34;t4g.micro&#34;
    instance_ami                           = &#34;ami-xxxxxxxxxxxx&#34;
    instance_root_block_device_volume_size = 10
    instance_app_name                      = &#34;app_name&#34;
    instance_app_env                       = &#34;prod&#34;
    healthcheck_endpoint                   = &#34;/healthcheck&#34;
    public_key                             = var.ssh_public_key
    vpc_id                                 = var.vpc_id
    jenkins_agent_sg_id                    = &#34;sg-jenkinsxxxxxxxx&#34;
    loadbalancer_subnets                   = [&#34;subnet-1&#34;, &#34;subnet-2&#34;, &#34;subnet-3&#34;]
    loadbalancer_tls_cert                  = &#34;arn:aws:acm:region-1:2356:certificate/XXXX-YYYY-ZZZZ&#34;
  }
</code></pre><p>This module doesn&rsquo;t only create 3 server instances, but also configures a loadbalancer that links to these instance through a target group and allow traffic in and out from these components, plus whitelists the access from Jenkins security group to the instance in order to perform the deployment. The implementation of this &ldquo;server_instances&rdquo; module involves 233 lines of Terraform code, 20 resource block usages, provides 16 customized variables, which is quite challenging to write from start to end with no mistake.</p><p>Since a &ldquo;terraform apply&rdquo; could affect to a numberous of infrastructure components, reviewing the plan carefully and only running the plan if every change looks correct is the practice recommended. Terraform has offered Terraform Enterprise, a paid platform to implement this practice automatically. However, if this is all we need from the Terraform Enterprise, a custom automation Jenkins pipeline might fulfill the job too.</p><h2 id=everything-starts-with-jenkins>Everything starts with Jenkins</h2><p>Jenkins plays an important role in automation and deployment. The Jenkins&rsquo; jobs or pipelines are represented in the code form, which makes it easier to maintain. The streamline monorepo setup makes it possible to have a single deployment pipeline with the predefined list of application&rsquo;s names. It reduces a lot of work to maintain the consistency of all Jenkins pipelines, as if there are separate deployment job for each service. On another hand, Jenkins can be used to run operational tasks such as refreshing AWS credentials, installing a software into a server, or even provisioning the infrastructure.</p><p>In general, to deploy an application, a number of steps are performed, which involves compiling the code, uploading the output to the artifact store, updating the code into server and restarting the app inside the server. Each of the step above may includes a few different tasks, for example, before compiling the code, some code evaluation&rsquo;s task are executed involving checking the dependencies&rsquo; vulnerabilities, code smells detection, verify the truthiness by running various kind of automation testing. If all the steps are passed, the code is compiled and tagged with a new version, and then stored inthe artifact store. It is ready to move on to the deploy stage.</p><p>There are usually multiple deployment environments. A deployment pipeline should define the order of each environment based on its criticalness. The most common setup has 2 environments: staging and production. The staging environment is used for the internal testing, while the production is for the real user to use the service. Since the production is more critical, the deployment pipeline should be able to block failed deployment to staging from being able to trigger into the production environment. It&rsquo;s quite often that after completing the deployment into each environment, the pipeline perform some readiness checks. The check could be simple as checking if the healthcheck endpoint returns a 200 status code or more complicated like running the smoke test, load test, and canary deployment.</p><p><img src=jenkins-deployment.png alt="Sample jenkins deployment pipeline"></p><p>On the other hand, most of the automated tasks could be handled by Jenkins. To illustrate what type of jobs could be run in Jenkins, this job handles some operational task related to issuing, revoking, rotating IAM credential&rsquo;s keys. The job takes a few parameters includes the action name, the iam user and the key id (for the delete action). Internally, it performs one of these actions with the given iam user and key, uses a pre-configured AWS keys with sufficient permissions. Typically, these permissions aren&rsquo;t granted to developers&rsquo; AWS accounts.</p><p><img src=jenkins-aws-iam-credentials.png alt="Jenkins AWS IAM credentials task"></p><p>As mentioned in the previous section, the Terraform pipeline could be implemented with 3 stages including planning, confirming and applying. This Jenkins job implements the Terraform flow as planning, confirmation, applying the plan. The job allows to run the plan with different modes like refresh only, normal and destroy mode. These operations are well fit to the current devops workload.</p><h2 id=ansible>Ansible</h2><p>If Jenkins is a computer then Ansible is the operating system that run in the computer.
It doesn&rsquo;t only implements the deployment script but also powers the build steps.
This section discusses some of Ansible&rsquo;s strengths in building these operation code.</p><p>Firstly, we use Ansible to instruct the application deployment.
The deployment script doesn&rsquo;t limit to the code deployment but also include the checklist to ensure the server is fully setup and operated. It includes tasks to check and install the missing packages, create configuration files and start supporting services, clean up the server after a successful deployment.</p><p>Secondly, instead of written the build script in pure Bash script, we use Ansible for the consistency and effeciency.
Ansible is designed with strong declarative syntax, which provides unlimited extendability. For example, when define the test running step, the command we think of is <code>npm run test</code>. This is good enough to run the test without dependencies, however, in the real project, it requires to run against dependencies and more than that there are different kinds of test for each application in the monorepo. Taking all of these requirements into account, the Ansible script to run the tests looks like below:</p><pre tabindex=0><code># build.yml
- include_tasks:
    file: &#34;{{ item }}&#34;
  with_items: &#34;{{ test_running_files }}&#34;

# vars file
test_running_files:
  - &#34;tests/run_unit_test.yml&#34;
  - &#34;tests/integration_test.yml&#34;
  - &#34;tests/e2e_test.yml&#34;

# integration_test.yml
- name: &#34;Run the integration test suite&#34;
  shell: &#34;docker-compose -f apps/{{ app_name }}/docker-compose.yml --project-directory ./ --project-name {{ app_name }} run integration-test {{ app_name }}&#34;
  args:
    chdir: &#34;{{ workspace_path }}&#34;
  register: test_output
  no_log: true
  ignore_errors: true
- name: &#34;Display test result&#34;
  include_tasks: &#34;../pretty_output.yml&#34;
  vars:
    output: &#34;{{ test_output }}&#34;
</code></pre><p>As the code shows how complicated the test running could be, Ansible is clearly a better choice to build the script due to its extendability. Each application config could override the test_running_files list if necessary.</p><h2 id=conclusion>Conclusion</h2><p>The post covers a few technologies together, hence it couldn&rsquo;t deep dive into each technology in details. The main idea is to bring an idea of how to design and build the infrastructure using these basic tools.
We could get our hands dirty in the future posts with more deeper details in using these tools.</p><ul></ul></div>Power by <a href=https://gohugo.io>Hugo</a> /
Theme <a href=https://github.com/J-Siu/hugo-theme-sk1/>SK1</a> by <a href=https://github.com/J-Siu/>J-Siu</a></body></html>